{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['question_id', 'answer_id', 'a_score_rel_q_score', 'answer_score',\n",
       "       'entities_matches', 'reputation', 'reply_by_author', 'len_answer_text',\n",
       "       'is_accepted', 'code_snippet_count', 'link_count', 'Hedges',\n",
       "       'Positive.Emotion', 'Negative.Emotion', 'Impersonal.Pronoun',\n",
       "       'Swearing', 'Negation', 'Filler.Pause', 'Informal.Title',\n",
       "       'Formal.Title', 'Could.You', 'Can.You', 'By.The.Way', 'Let.Me.Know',\n",
       "       'Goodbye', 'For.Me', 'For.You', 'Reasoning', 'Reassurance',\n",
       "       'Ask.Agency', 'Give.Agency', 'Hello', 'Please', 'First.Person.Plural',\n",
       "       'First.Person.Single', 'Second.Person', 'Agreement', 'Acknowledgement',\n",
       "       'Subjectivity', 'Bare.Command', 'WH.Questions', 'YesNo.Questions',\n",
       "       'Gratitude', 'Apology', 'Truth.Intensifier', 'Affirmation',\n",
       "       'Adverb.Just', 'Conjunction.Start'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"datasets/features_with_politeness.csv\", lineterminator='\\n', encoding=\"ISO-8859-1\").dropna()\n",
    "print(len(data))\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7467\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>answer_id</th>\n",
       "      <th>a_score_rel_q_score</th>\n",
       "      <th>answer_score</th>\n",
       "      <th>entities_matches</th>\n",
       "      <th>reputation</th>\n",
       "      <th>reply_by_author</th>\n",
       "      <th>len_answer_text</th>\n",
       "      <th>is_accepted</th>\n",
       "      <th>code_snippet_count</th>\n",
       "      <th>...</th>\n",
       "      <th>Bare.Command</th>\n",
       "      <th>WH.Questions</th>\n",
       "      <th>YesNo.Questions</th>\n",
       "      <th>Gratitude</th>\n",
       "      <th>Apology</th>\n",
       "      <th>Truth.Intensifier</th>\n",
       "      <th>Affirmation</th>\n",
       "      <th>Adverb.Just</th>\n",
       "      <th>Conjunction.Start</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1574</th>\n",
       "      <td>67239221</td>\n",
       "      <td>67239259</td>\n",
       "      <td>0.175378</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.425816</td>\n",
       "      <td>-0.421044</td>\n",
       "      <td>-0.185487</td>\n",
       "      <td>-0.495355</td>\n",
       "      <td>1.837498</td>\n",
       "      <td>3.771277</td>\n",
       "      <td>...</td>\n",
       "      <td>2.417250</td>\n",
       "      <td>-0.194408</td>\n",
       "      <td>-0.276662</td>\n",
       "      <td>-0.114404</td>\n",
       "      <td>-0.083912</td>\n",
       "      <td>-0.325376</td>\n",
       "      <td>-0.169873</td>\n",
       "      <td>-0.396733</td>\n",
       "      <td>-0.407037</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547</th>\n",
       "      <td>67237155</td>\n",
       "      <td>67237241</td>\n",
       "      <td>0.179703</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.425816</td>\n",
       "      <td>-0.308212</td>\n",
       "      <td>-0.185487</td>\n",
       "      <td>-0.130865</td>\n",
       "      <td>1.837498</td>\n",
       "      <td>1.617715</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.414020</td>\n",
       "      <td>-0.194408</td>\n",
       "      <td>-0.276662</td>\n",
       "      <td>-0.114404</td>\n",
       "      <td>-0.083912</td>\n",
       "      <td>-0.325376</td>\n",
       "      <td>-0.169873</td>\n",
       "      <td>1.412784</td>\n",
       "      <td>-0.407037</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>67237061</td>\n",
       "      <td>67237260</td>\n",
       "      <td>0.175378</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.425816</td>\n",
       "      <td>1.079331</td>\n",
       "      <td>-0.185487</td>\n",
       "      <td>1.586562</td>\n",
       "      <td>1.837498</td>\n",
       "      <td>-0.535847</td>\n",
       "      <td>...</td>\n",
       "      <td>1.001615</td>\n",
       "      <td>-0.194408</td>\n",
       "      <td>-0.276662</td>\n",
       "      <td>-0.114404</td>\n",
       "      <td>-0.083912</td>\n",
       "      <td>-0.325376</td>\n",
       "      <td>-0.169873</td>\n",
       "      <td>3.222301</td>\n",
       "      <td>2.646720</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>67236923</td>\n",
       "      <td>67236978</td>\n",
       "      <td>0.184029</td>\n",
       "      <td>5</td>\n",
       "      <td>4.109973</td>\n",
       "      <td>0.011456</td>\n",
       "      <td>-0.185487</td>\n",
       "      <td>-0.841312</td>\n",
       "      <td>1.837498</td>\n",
       "      <td>0.540934</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.414020</td>\n",
       "      <td>-0.194408</td>\n",
       "      <td>-0.276662</td>\n",
       "      <td>-0.114404</td>\n",
       "      <td>-0.083912</td>\n",
       "      <td>-0.325376</td>\n",
       "      <td>-0.169873</td>\n",
       "      <td>-0.396733</td>\n",
       "      <td>-0.407037</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>67236923</td>\n",
       "      <td>67237336</td>\n",
       "      <td>0.173215</td>\n",
       "      <td>0</td>\n",
       "      <td>0.708131</td>\n",
       "      <td>-0.419148</td>\n",
       "      <td>-0.185487</td>\n",
       "      <td>-0.866023</td>\n",
       "      <td>-0.544218</td>\n",
       "      <td>0.540934</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.414020</td>\n",
       "      <td>-0.194408</td>\n",
       "      <td>-0.276662</td>\n",
       "      <td>-0.114404</td>\n",
       "      <td>-0.083912</td>\n",
       "      <td>-0.325376</td>\n",
       "      <td>-0.169873</td>\n",
       "      <td>-0.396733</td>\n",
       "      <td>1.119841</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>826</td>\n",
       "      <td>834</td>\n",
       "      <td>0.123471</td>\n",
       "      <td>-4</td>\n",
       "      <td>-0.425816</td>\n",
       "      <td>-0.343975</td>\n",
       "      <td>-0.185487</td>\n",
       "      <td>-0.104095</td>\n",
       "      <td>-0.544218</td>\n",
       "      <td>-0.535847</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.414020</td>\n",
       "      <td>-0.194408</td>\n",
       "      <td>-0.276662</td>\n",
       "      <td>-0.114404</td>\n",
       "      <td>-0.083912</td>\n",
       "      <td>-0.325376</td>\n",
       "      <td>-0.169873</td>\n",
       "      <td>-0.396733</td>\n",
       "      <td>-0.407037</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>810</td>\n",
       "      <td>829</td>\n",
       "      <td>0.145099</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.425816</td>\n",
       "      <td>0.260550</td>\n",
       "      <td>-0.185487</td>\n",
       "      <td>1.304442</td>\n",
       "      <td>-0.544218</td>\n",
       "      <td>-0.535847</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.414020</td>\n",
       "      <td>-0.194408</td>\n",
       "      <td>1.337252</td>\n",
       "      <td>-0.114404</td>\n",
       "      <td>-0.083912</td>\n",
       "      <td>-0.325376</td>\n",
       "      <td>-0.169873</td>\n",
       "      <td>3.222301</td>\n",
       "      <td>1.119841</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>810</td>\n",
       "      <td>820</td>\n",
       "      <td>0.145099</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.425816</td>\n",
       "      <td>0.583781</td>\n",
       "      <td>-0.185487</td>\n",
       "      <td>0.464262</td>\n",
       "      <td>1.837498</td>\n",
       "      <td>0.540934</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.414020</td>\n",
       "      <td>-0.194408</td>\n",
       "      <td>1.337252</td>\n",
       "      <td>-0.114404</td>\n",
       "      <td>-0.083912</td>\n",
       "      <td>1.706875</td>\n",
       "      <td>-0.169873</td>\n",
       "      <td>1.412784</td>\n",
       "      <td>-0.407037</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>810</td>\n",
       "      <td>818</td>\n",
       "      <td>0.136448</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.425816</td>\n",
       "      <td>-0.160983</td>\n",
       "      <td>5.391205</td>\n",
       "      <td>-0.159695</td>\n",
       "      <td>-0.544218</td>\n",
       "      <td>-0.535847</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.414020</td>\n",
       "      <td>-0.194408</td>\n",
       "      <td>-0.276662</td>\n",
       "      <td>-0.114404</td>\n",
       "      <td>-0.083912</td>\n",
       "      <td>-0.325376</td>\n",
       "      <td>-0.169873</td>\n",
       "      <td>1.412784</td>\n",
       "      <td>-0.407037</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>810</td>\n",
       "      <td>870</td>\n",
       "      <td>0.136448</td>\n",
       "      <td>1</td>\n",
       "      <td>0.708131</td>\n",
       "      <td>-0.160983</td>\n",
       "      <td>5.391205</td>\n",
       "      <td>0.066824</td>\n",
       "      <td>-0.544218</td>\n",
       "      <td>-0.535847</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.414020</td>\n",
       "      <td>-0.194408</td>\n",
       "      <td>-0.276662</td>\n",
       "      <td>-0.114404</td>\n",
       "      <td>-0.083912</td>\n",
       "      <td>-0.325376</td>\n",
       "      <td>-0.169873</td>\n",
       "      <td>1.412784</td>\n",
       "      <td>1.119841</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10162 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      question_id  answer_id  a_score_rel_q_score  answer_score  \\\n",
       "1574     67239221   67239259             0.175378             1   \n",
       "1547     67237155   67237241             0.179703             0   \n",
       "1343     67237061   67237260             0.175378             0   \n",
       "1150     67236923   67236978             0.184029             5   \n",
       "1149     67236923   67237336             0.173215             0   \n",
       "...           ...        ...                  ...           ...   \n",
       "783           826        834             0.123471            -4   \n",
       "774           810        829             0.145099             5   \n",
       "775           810        820             0.145099             5   \n",
       "772           810        818             0.136448             1   \n",
       "773           810        870             0.136448             1   \n",
       "\n",
       "      entities_matches  reputation  reply_by_author  len_answer_text  \\\n",
       "1574         -0.425816   -0.421044        -0.185487        -0.495355   \n",
       "1547         -0.425816   -0.308212        -0.185487        -0.130865   \n",
       "1343         -0.425816    1.079331        -0.185487         1.586562   \n",
       "1150          4.109973    0.011456        -0.185487        -0.841312   \n",
       "1149          0.708131   -0.419148        -0.185487        -0.866023   \n",
       "...                ...         ...              ...              ...   \n",
       "783          -0.425816   -0.343975        -0.185487        -0.104095   \n",
       "774          -0.425816    0.260550        -0.185487         1.304442   \n",
       "775          -0.425816    0.583781        -0.185487         0.464262   \n",
       "772          -0.425816   -0.160983         5.391205        -0.159695   \n",
       "773           0.708131   -0.160983         5.391205         0.066824   \n",
       "\n",
       "      is_accepted  code_snippet_count  ...  Bare.Command  WH.Questions  \\\n",
       "1574     1.837498            3.771277  ...      2.417250     -0.194408   \n",
       "1547     1.837498            1.617715  ...     -0.414020     -0.194408   \n",
       "1343     1.837498           -0.535847  ...      1.001615     -0.194408   \n",
       "1150     1.837498            0.540934  ...     -0.414020     -0.194408   \n",
       "1149    -0.544218            0.540934  ...     -0.414020     -0.194408   \n",
       "...           ...                 ...  ...           ...           ...   \n",
       "783     -0.544218           -0.535847  ...     -0.414020     -0.194408   \n",
       "774     -0.544218           -0.535847  ...     -0.414020     -0.194408   \n",
       "775      1.837498            0.540934  ...     -0.414020     -0.194408   \n",
       "772     -0.544218           -0.535847  ...     -0.414020     -0.194408   \n",
       "773     -0.544218           -0.535847  ...     -0.414020     -0.194408   \n",
       "\n",
       "      YesNo.Questions  Gratitude   Apology  Truth.Intensifier  Affirmation  \\\n",
       "1574        -0.276662  -0.114404 -0.083912          -0.325376    -0.169873   \n",
       "1547        -0.276662  -0.114404 -0.083912          -0.325376    -0.169873   \n",
       "1343        -0.276662  -0.114404 -0.083912          -0.325376    -0.169873   \n",
       "1150        -0.276662  -0.114404 -0.083912          -0.325376    -0.169873   \n",
       "1149        -0.276662  -0.114404 -0.083912          -0.325376    -0.169873   \n",
       "...               ...        ...       ...                ...          ...   \n",
       "783         -0.276662  -0.114404 -0.083912          -0.325376    -0.169873   \n",
       "774          1.337252  -0.114404 -0.083912          -0.325376    -0.169873   \n",
       "775          1.337252  -0.114404 -0.083912           1.706875    -0.169873   \n",
       "772         -0.276662  -0.114404 -0.083912          -0.325376    -0.169873   \n",
       "773         -0.276662  -0.114404 -0.083912          -0.325376    -0.169873   \n",
       "\n",
       "      Adverb.Just  Conjunction.Start  rank  \n",
       "1574    -0.396733          -0.407037   1.0  \n",
       "1547     1.412784          -0.407037   1.0  \n",
       "1343     3.222301           2.646720   1.0  \n",
       "1150    -0.396733          -0.407037   1.0  \n",
       "1149    -0.396733           1.119841   2.0  \n",
       "...           ...                ...   ...  \n",
       "783     -0.396733          -0.407037   7.0  \n",
       "774      3.222301           1.119841   1.5  \n",
       "775      1.412784          -0.407037   1.5  \n",
       "772      1.412784          -0.407037   3.5  \n",
       "773      1.412784           1.119841   3.5  \n",
       "\n",
       "[10162 rows x 49 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_data = data.copy()\n",
    "scaler = StandardScaler()\n",
    "cols = (data.columns.tolist())\n",
    "del cols[0:2]\n",
    "cols.remove('answer_score')\n",
    "b_data[cols] = scaler.fit_transform(b_data[cols])\n",
    "b_data.sort_values(by=['question_id','answer_score'], ascending=False, inplace=True)\n",
    "#b_data[\"rank\"] = b_data.groupby(\"question_id\")[\"answer_score\"].rank(ascending=False)\n",
    "print(max(b_data['answer_score']))\n",
    "b_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63.5485908  45.5656156  11.28121403 ... 46.50649973 -4.35833012\n",
      " 10.30542682]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'importances_mean': array([ 1.46310433e-03,  1.55852417e-03, -6.20229008e-04, -6.36132316e-05,\n",
       "         4.05534351e-03,  2.01972010e-03,  3.27608142e-03,  3.95992366e-03,\n",
       "         3.33969466e-03,  6.04325700e-04,  7.79262087e-04, -4.93002545e-04,\n",
       "         3.92811705e-03,  4.77099237e-05,  1.60623410e-03,  4.23027990e-03,\n",
       "         9.70101781e-04,  1.06552163e-03,  1.59033079e-05, -3.49872774e-04,\n",
       "        -1.59033079e-05,  1.90839695e-04,  6.20229008e-04, -1.59033079e-05,\n",
       "        -3.18066158e-05, -6.20229008e-04, -4.24618321e-03,  3.33969466e-04,\n",
       "        -6.99745547e-04,  8.42875318e-04, -7.95165394e-05,  1.87659033e-03,\n",
       "         5.24809160e-04,  1.59033079e-05,  2.17875318e-03,  1.49491094e-03,\n",
       "         3.18066158e-05, -2.33778626e-03, -3.18066158e-05, -3.97582697e-04,\n",
       "         9.86005089e-04,  1.74936387e-04,  1.49491094e-03,  6.36132316e-05]),\n",
       " 'importances_std': array([2.54532430e-03, 3.11623685e-03, 2.09108845e-03, 1.35915144e-03,\n",
       "        3.32089858e-03, 2.61516347e-03, 1.77974998e-03, 2.48259754e-03,\n",
       "        1.44186094e-03, 2.32406047e-03, 3.42157738e-03, 2.88459878e-04,\n",
       "        3.11075369e-03, 2.56925802e-04, 1.58467506e-03, 2.10405031e-03,\n",
       "        7.74051751e-04, 5.86700596e-04, 3.13662260e-04, 4.59822910e-04,\n",
       "        8.56419340e-05, 5.45369854e-04, 7.70777406e-04, 2.33237070e-03,\n",
       "        3.46969215e-04, 5.79762282e-04, 3.47846463e-03, 6.29939641e-04,\n",
       "        5.74283400e-04, 6.71149741e-04, 1.46836715e-03, 1.56758544e-03,\n",
       "        8.38663923e-04, 6.57443594e-04, 1.36444450e-03, 1.71902915e-03,\n",
       "        1.00782312e-03, 2.15963174e-03, 7.78450270e-04, 3.27855091e-04,\n",
       "        1.78060242e-03, 5.43744307e-04, 1.56176637e-03, 4.31281759e-03]),\n",
       " 'importances': array([[ 0.0019084 ,  0.0014313 ,  0.0019084 , ...,  0.0009542 ,\n",
       "         -0.0028626 , -0.0009542 ],\n",
       "        [ 0.00524809,  0.0004771 , -0.0004771 , ...,  0.00381679,\n",
       "         -0.0023855 ,  0.00667939],\n",
       "        [-0.0014313 , -0.0014313 ,  0.0014313 , ...,  0.        ,\n",
       "         -0.0004771 ,  0.        ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.0004771 ,  0.0004771 , ...,  0.        ,\n",
       "          0.0004771 ,  0.        ],\n",
       "        [ 0.0019084 ,  0.00381679, -0.0009542 , ...,  0.        ,\n",
       "          0.00333969,  0.0014313 ],\n",
       "        [ 0.        , -0.00477099,  0.0004771 , ..., -0.00429389,\n",
       "          0.00477099, -0.00763359]])}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "m_data = b_data.drop(['answer_id', 'is_accepted'], axis=1)\n",
    "test_set = m_data.iloc[0:2096].copy()\n",
    "\n",
    "test_label = test_set['answer_score']\n",
    "\n",
    "\n",
    "test = test_set.drop(['question_id', 'answer_score'], axis=1).to_numpy()\n",
    "train_set = m_data.iloc[m_data.index[2096:]]\n",
    "train_label = train_set['answer_score']\n",
    "train = train_set.drop(['question_id', 'answer_score'], axis=1).to_numpy()\n",
    "reg = LinearRegression().fit(train, train_label)\n",
    "predictions = reg.predict(test)\n",
    "print(predictions)\n",
    "r = permutation_importance(clf, test, test_label,n_repeats=30,random_state=0)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 1 0 1]\n",
      " [0 1 0 ... 1 1 1]\n",
      " ...\n",
      " [1 0 0 ... 0 0 1]\n",
      " [0 1 0 ... 0 0 1]\n",
      " [1 0 0 ... 0 0 1]]\n",
      "[2 3 3 ... 1 3 2]\n",
      "1574    1.0\n",
      "1547    1.0\n",
      "1343    1.0\n",
      "1150    2.0\n",
      "1149    1.0\n",
      "       ... \n",
      "8392    6.5\n",
      "8386    2.0\n",
      "8373    1.0\n",
      "8374    3.0\n",
      "8375    2.0\n",
      "Name: pred, Length: 2096, dtype: float64\n",
      "[771 779  86 460]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.39373488851050986, pvalue=1.1205504605537895e-78)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#b_data['is_accepted'] = b_data['is_accepted'].astype(int).to_numpy()\n",
    "#print(np.bincount(target))\n",
    "# Drop non-discrete columns and convert columns\n",
    "\n",
    "\n",
    "to_binary_cols = ['Hedges', 'Positive.Emotion', 'Negative.Emotion',\n",
    "       'Impersonal.Pronoun', 'Swearing', 'Negation', 'Filler.Pause',\n",
    "       'Informal.Title', 'Formal.Title', 'Could.You', 'Can.You', 'By.The.Way',\n",
    "       'Let.Me.Know', 'Goodbye', 'For.Me', 'For.You', 'Reasoning',\n",
    "       'Reassurance', 'Ask.Agency', 'Give.Agency', 'Hello', 'Please',\n",
    "       'First.Person.Plural', 'First.Person.Single', 'Second.Person',\n",
    "       'Agreement', 'Acknowledgement', 'Subjectivity', 'Bare.Command',\n",
    "       'WH.Questions', 'YesNo.Questions', 'Gratitude', 'Apology',\n",
    "       'Truth.Intensifier', 'Affirmation', 'Adverb.Just', 'Conjunction.Start', \n",
    "        'code_snippet_count', 'link_count', 'entities_matches']\n",
    "\n",
    "for col in to_binary_cols:\n",
    "    b_data[col] = b_data[col].apply(lambda x: 0 if x == 0 else 1)\n",
    "    \n",
    "avg_len = b_data['len_answer_text'].mean()\n",
    "avg_rep = b_data['reputation'].mean()\n",
    "\n",
    "b_data['len_answer_text'] = b_data['len_answer_text'].apply(lambda x: 0 if x < avg_len else 1)\n",
    "b_data['reputation'] = b_data['reputation'].apply(lambda x: 0 if x < avg_rep else 1)\n",
    "b_data['has_more_upvotes_than_ans'] = b_data['a_score_rel_q_score'].apply(lambda x: 0 if x < 0 else 1)\n",
    "b_data['reply_by_author'] = b_data[\"reply_by_author\"].astype(int)\n",
    "\n",
    "bins = [-20, 0, 2, 5, 7500]\n",
    "target,width_bins = pd.cut(b_data['answer_score'], bins, retbins=True, duplicates='drop')\n",
    "\n",
    "width_binned_counts = target.value_counts()\n",
    "\n",
    "b_data['answer_score'] = target.cat.codes\n",
    "\n",
    "\n",
    "m_data = b_data.drop(['answer_id', 'a_score_rel_q_score', 'is_accepted'], axis=1)\n",
    "\n",
    "test_set = m_data.iloc[0:2096].copy()\n",
    "\n",
    "test_label = test_set['answer_score']\n",
    "\n",
    "\n",
    "test = test_set.drop(['question_id', 'rank','answer_score'], axis=1).to_numpy()\n",
    "train_set = m_data.iloc[m_data.index[2096:]]\n",
    "train_label = train_set['answer_score']\n",
    "train = train_set.drop(['question_id', 'rank','answer_score'], axis=1).to_numpy()\n",
    "print(test)\n",
    "\n",
    "clf = BernoulliNB()\n",
    "clf.fit(train, train_label)\n",
    "predictions = clf.predict(test)\n",
    "print(predictions)\n",
    "\n",
    "test_set['pred'] = predictions\n",
    "rankings = test_set.groupby(\"question_id\")[\"pred\"].rank()\n",
    "print(rankings)\n",
    "\n",
    "print(np.bincount(predictions))\n",
    "\n",
    "\n",
    "r = permutation_importance(clf, test, test_label,n_repeats=30,random_state=0)\n",
    "\n",
    "\n",
    "stats.spearmanr(rankings, test_set['rank'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 ... 1 3 2]\n",
      "[663 904  84 445]\n",
      "[ 3.49872774e-03  4.26208651e-03  2.56043257e-03  2.13740458e-02\n",
      "  1.40585242e-02  6.85432570e-03  3.30788804e-03 -1.31997455e-03\n",
      " -1.25636132e-03 -2.54452926e-04 -1.00190840e-03  6.77480916e-03\n",
      "  5.24809160e-04  1.49491094e-03 -4.13486005e-04 -5.72519084e-04\n",
      "  8.90585242e-04 -1.27226463e-04  3.18066158e-05 -7.40148683e-18\n",
      " -9.54198473e-04 -7.63358779e-04 -7.40148683e-18  1.00190840e-03\n",
      "  7.63358779e-04 -1.60623410e-03  2.76717557e-03  1.14503817e-03\n",
      "  5.08905852e-04 -2.79898219e-03  3.16475827e-03  2.70356234e-04\n",
      "  9.06488550e-04  1.55852417e-03 -1.35178117e-03  4.93002545e-04\n",
      " -1.46310433e-03 -1.03371501e-03  1.01781170e-03  9.70101781e-04\n",
      "  1.46310433e-03 -1.57442748e-03 -1.01781170e-03  3.51304071e-02]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.7037136863307422, pvalue=3.5324927128e-313)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(multi_class='multinomial',solver ='newton-cg').fit(train,train_label)\n",
    "predictions = clf.predict(test)\n",
    "print(predictions)\n",
    "print(np.bincount(predictions))\n",
    "\n",
    "test_set['pred'] = predictions\n",
    "rankings = test_set.groupby(\"question_id\")[\"pred\"].rank(ascending=False)\n",
    "\n",
    "r = permutation_importance(clf, test, test_label,n_repeats=30,random_state=0)\n",
    "print(r.importances_mean)\n",
    "\n",
    "stats.spearmanr(rankings, test_set['rank'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
